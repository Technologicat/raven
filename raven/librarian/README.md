<p align="center">
<img src="../../img/logo.png" alt="Logo of Raven" height="200"/> <br/>
</p>

-----

<p align="center">
<img src="../../img/screenshot-librarian.png" alt="Screenshot of Raven-librarian" width="800"/> <br/>
<i>Raven-librarian is an LLM frontend with natively nonlinear chat history, a document database, and a talking AI avatar.</i>
</p>

<!-- markdown-toc start - Don't edit this section. Run M-x markdown-toc-refresh-toc -->
**Table of Contents**

- [Introduction](#introduction)
- [Features](#features)
    - [Nonlinear history](#nonlinear-history)
    - [Document database](#document-database)
    - [Tools](#tools)
- [Voice mode](#voice-mode)
- [GUI walkthrough](#gui-walkthrough)
- [Configuration](#configuration)
    - [Server connections](#server-connections)
    - [Voice mode](#voice-mode-1)
    - [System prompt, AI character personality, communication style](#system-prompt-ai-character-personality-communication-style)
    - [AI avatar](#ai-avatar)
- [Appendix: Brief notes on how to set up a local LLM](#appendix-brief-notes-on-how-to-set-up-a-local-llm)

<!-- markdown-toc end -->

# Introduction

:exclamation: *This document is a very early WIP, with many placeholders.* :exclamation:

- what Librarian is

# Features

- nonlinear history, RAG, tools

## Nonlinear history

- tree structure, root node at the system prompt
  - system prompt -> AI's greeting -> user's first message -> ... insert figure here!
  - a node may have several children, but has exactly one parent
    - the only exception is the system prompt node, whose parent is `None`
  - the linearized history for the current chat can be generated by following the parent links, up to the root
    - it follows that a chat branch is just its HEAD pointer (roughly, like in `git`)
    - this makes some actions cheap:
      - starting a new chat only resets the HEAD pointer to the AI's greeting
      - branching only sets the HEAD pointer to the node where you branch at

- why?
  - an LLM is a stochastic model conditioned on the prefix (the chat so far); there is always variance in the output
  - an LLM only "remembers" in two ways:
    - static knowledge stored in weights, from training
    - **the text in the context**
  - a tree structure facilitates *context engineering* the chat in an agile manner
    - ...in addition to any context engineering the LLM frontend already does, such as automatic document database searches
  - you can reroll the AI's response at any point in the chat, *while stashing all the old outputs*
    - this is very, very useful

- how a nonlinear history is useful, in practice
  - often, one does not get a perfect response on the first try, but old responses may contain useful parts
    - so reroll and then edit manually to combine the best
  - epistemic analysis:
    - if an LLM states the same thing over and over when rerolled, it believes in what it said
      - ...regardless of whether the "fact" is true or not
    - if it gives a wildly different response each time when rerolled, then it didn't actually know, didn't notice that it didn't know, and confabulated ("*hallucinated*") something random
  - visit alternative branches later

- the history tree is stored (by default) in `~/.config/raven/llmclient/data.json`
  - unit of storage: chat message, with links (parent, and possible children)
  - more exactly:
    - each node stores metadata and a versioned payload
    - payload versioning facilitates light editing that doesn't change the flow of the chat (e.g. fixing typos after the fact)
      - i.e. edits where the old responses still make sense in the updated context
      - editing doesn't touch the links
    - for large changes, such as replacing a question with a different one (so that the whole downstream chat needs to change to make sense), use **branching**

## Document database

- for now, plain text only
  - Any plain text format that your LLM can read is fine (e.g. `.txt`, `.md`, `.bib`)
  - To extract text from PDF files, use `pdftotext`; or failing that, `ocrmypdf`
  - We plan to integrate PDF and HTML importers later
- where to put documents, what Librarian does with them
  - can be configured in `raven.librarian.config`
  - default is `~/.config/raven/llmclient/documents`
- search index auto-update mechanism (offline & online update)

The search index automatically reflects the current state of the document database. Removing the documents from the document database also automatically removes them from the index.

It is possible to manually force rebuilding the index, by deleting the whole index while *Raven-librarian* is not running, and then starting *Raven-librarian*.

## Tools

- What tool use is
- Tools provided
  - Websearch
  - We plan to expand this later

# Voice mode

Librarian features a lipsynced talking AI avatar, as well as speech recognition for text input. These features combine into a **voice mode**.

For configuring the AI's voice, see [configuration](#configuration) below.

The AI avatar can optionally display machine-translated subtitles for its speech.

To speak to the AI, click the **mic button**. Once you are done talking, click again, or wait until the automatic silence detector ends the recording.

- voice mode (STT, TTS)

# GUI walkthrough

- where features are
  - why some buttons are at the bottom
  - why some buttons are below every chat message
    - why some buttons only appear for certain types of chat message
- what the GUI toggles below the avatar do
  - the toggles are persistent, default place to store the app state is `~/.config/raven/llmclient/state.json`

# Configuration

As explained in the main README, configuration is currently fed in as several Python modules that exist specifically as configuration files.

## Server connections

- LLM backend URL and API key: [`raven.librarian.config`](config.py)
  - Whether you need an API key depends on your LLM. By default, a local installation of [oobabooga/text-generation-webui](https://github.com/oobabooga/text-generation-webui) does **not** use an API key.

- Raven-server URL and API key: [`raven.client.config`](../client/config.py)
  - By default, *Raven-server* does **not** use an API key.
  - If you want to set up an API key for your *Raven-server*, see the `--secure` command line option of `raven-server`.
    - Note that this is a very light form of authentication that only requires providing a shared secret (the API key). The API key is transmitted in plain text.
    - Importantly, the `--secure` mode does **not** encrypt the connection.

## Voice mode

The AI's voice is configured in the AI avatar configuration.

- TTS is part of avatar config in [`raven.librarian.config`](config.py)
- STT model is configured in [`raven.server.config`](../server/config.py)
- for subtitles, machine translation model is selected in [`raven.server.config`](../server/config.py)
- audio devices are selected in [`raven.client.config`](../client/config.py); see also `raven-check-audio-devices` command-line tool to list devices present on your system

## System prompt, AI character personality, communication style

- [`raven.librarian.config`](config.py)
- technically, just a system prompt - this goes to the beginning of every chat
- but in practice, useful to think of it as *system prompt + AI character card* (the default out-of-the-box configuration does this)

## AI avatar

- character choice in [`raven.librarian.config`](config.py)
  - the AI avatar and the AI character name/personality are set up separately
  - to avoid surprises, make sure these match
- AI voice (TTS) is also configured in [`raven.librarian.config`](config.py)
- Use the GUI app `raven-avatar-settings-editor` to create or edit the `animator.json` configuration file (avatar video postprocessor settings)

# Appendix: Brief notes on how to set up a local LLM

- recommended for privacy
- setting up a local LLM with text-generation-webui (where to find install instructions; links to recommended models on HF; important command-line options)
